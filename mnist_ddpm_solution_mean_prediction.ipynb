{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trnhatnam/PML_MNIST_Diffusion/blob/main/mnist_ddpm_solution_mean_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UfQDnOUcK4iG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OimlcBLxYkqc",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# This UNET-style prediction model was originally included as part of the Score-based generative modelling tutorial\n",
        "# by Yang Song et al: https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class GaussianFourierProjection(nn.Module):\n",
        "  \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
        "  def __init__(self, embed_dim, scale=30.):\n",
        "    super().__init__()\n",
        "    # Randomly sample weights during initialization. These weights are fixed\n",
        "    # during optimization and are not trainable.\n",
        "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
        "  def forward(self, x):\n",
        "    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
        "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(input_dim, output_dim)\n",
        "  def forward(self, x):\n",
        "    return self.dense(x)[..., None, None]\n",
        "\n",
        "\n",
        "class ScoreNet(nn.Module):\n",
        "  \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
        "\n",
        "  def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
        "    \"\"\"Initialize a time-dependent score-based network.\n",
        "\n",
        "    Args:\n",
        "      marginal_prob_std: A function that takes time t and gives the standard\n",
        "        deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
        "      channels: The number of channels for feature maps of each resolution.\n",
        "      embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Gaussian random feature embedding layer for time\n",
        "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
        "         nn.Linear(embed_dim, embed_dim))\n",
        "    # Encoding layers where the resolution decreases\n",
        "    self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
        "    self.dense1 = Dense(embed_dim, channels[0])\n",
        "    self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
        "    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
        "    self.dense2 = Dense(embed_dim, channels[1])\n",
        "    self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
        "    self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
        "    self.dense3 = Dense(embed_dim, channels[2])\n",
        "    self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
        "    self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
        "    self.dense4 = Dense(embed_dim, channels[3])\n",
        "    self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
        "\n",
        "    # Decoding layers where the resolution increases\n",
        "    self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
        "    self.dense5 = Dense(embed_dim, channels[2])\n",
        "    self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
        "    self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)\n",
        "    self.dense6 = Dense(embed_dim, channels[1])\n",
        "    self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
        "    self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)\n",
        "    self.dense7 = Dense(embed_dim, channels[0])\n",
        "    self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
        "    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
        "\n",
        "    # The swish activation function\n",
        "    self.act = lambda x: x * torch.sigmoid(x)\n",
        "    self.marginal_prob_std = marginal_prob_std\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    # Obtain the Gaussian random feature embedding for t\n",
        "    embed = self.act(self.embed(t))\n",
        "    # Encoding path\n",
        "    h1 = self.conv1(x)\n",
        "    ## Incorporate information from t\n",
        "    h1 += self.dense1(embed)\n",
        "    ## Group normalization\n",
        "    h1 = self.gnorm1(h1)\n",
        "    h1 = self.act(h1)\n",
        "    h2 = self.conv2(h1)\n",
        "    h2 += self.dense2(embed)\n",
        "    h2 = self.gnorm2(h2)\n",
        "    h2 = self.act(h2)\n",
        "    h3 = self.conv3(h2)\n",
        "    h3 += self.dense3(embed)\n",
        "    h3 = self.gnorm3(h3)\n",
        "    h3 = self.act(h3)\n",
        "    h4 = self.conv4(h3)\n",
        "    h4 += self.dense4(embed)\n",
        "    h4 = self.gnorm4(h4)\n",
        "    h4 = self.act(h4)\n",
        "\n",
        "    # Decoding path\n",
        "    h = self.tconv4(h4)\n",
        "    ## Skip connection from the encoding path\n",
        "    h += self.dense5(embed)\n",
        "    h = self.tgnorm4(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv3(torch.cat([h, h3], dim=1))\n",
        "    h += self.dense6(embed)\n",
        "    h = self.tgnorm3(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv2(torch.cat([h, h2], dim=1))\n",
        "    h += self.dense7(embed)\n",
        "    h = self.tgnorm2(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv1(torch.cat([h, h1], dim=1))\n",
        "\n",
        "    # Normalize output\n",
        "    h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "41Mv068QK4iI"
      },
      "outputs": [],
      "source": [
        "# ExponentialMovingAverage implementation as used in pytorch vision\n",
        "# https://github.com/pytorch/vision/blob/main/references/classification/utils.py#L159\n",
        "\n",
        "# BSD 3-Clause License\n",
        "\n",
        "# Copyright (c) Soumith Chintala 2016,\n",
        "# All rights reserved.\n",
        "\n",
        "# Redistribution and use in source and binary forms, with or without\n",
        "# modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "# * Redistributions of source code must retain the above copyright notice, this\n",
        "#   list of conditions and the following disclaimer.\n",
        "\n",
        "# * Redistributions in binary form must reproduce the above copyright notice,\n",
        "#   this list of conditions and the following disclaimer in the documentation\n",
        "#   and/or other materials provided with the distribution.\n",
        "\n",
        "# * Neither the name of the copyright holder nor the names of its\n",
        "#   contributors may be used to endorse or promote products derived from\n",
        "#   this software without specific prior written permission.\n",
        "\n",
        "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
        "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
        "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
        "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
        "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "\n",
        "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
        "    \"\"\"Maintains moving averages of model parameters using an exponential decay.\n",
        "    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``\n",
        "    `torch.optim.swa_utils.AveragedModel <https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies>`_\n",
        "    is used to compute the EMA.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, decay, device=\"cpu\"):\n",
        "        def ema_avg(avg_model_param, model_param, num_averaged):\n",
        "            return decay * avg_model_param + (1 - decay) * model_param\n",
        "\n",
        "        super().__init__(model, device, ema_avg, use_buffers=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 995,
          "referenced_widgets": [
            "29502e060dbb4b0db2feda3b2b9f0600",
            "a92fdede28114771868307c2695f1014",
            "1d8c515548034a26acb475a21e49ad99",
            "3510307ac24b449ab9853ecf2661d119",
            "3631c39d8fc04c0e8f1830c150be168b",
            "098ad33be85c4027b6bae3cb2dc33b67",
            "a2510616091d44779a4072e101b4a4c2",
            "06fe2a00be72439cad2f0a6586966fde",
            "fdd86dff33a44d7e8ca4134463933af1",
            "3ddedb88f44a452dafbea68fac60d427",
            "5b80b05a8733492f9bf3fc8a350bfa05"
          ]
        },
        "id": "mcoxR2ajYkqe",
        "outputId": "bafd0021-7365-4d41-f547-b6adb2e6cdaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 500kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.47MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.77MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1175 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29502e060dbb4b0db2feda3b2b9f0600"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-39b37b8d8e1a>\u001b[0m in \u001b[0;36m<cell line: 285>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;31m# Call training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m train(model, optimizer, scheduler, dataloader_train,\n\u001b[0m\u001b[1;32m    286\u001b[0m       epochs=epochs, device=device, ema=True, per_epoch_callback=reporter)\n",
            "\u001b[0;32m<ipython-input-4-39b37b8d8e1a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, dataloader, epochs, device, ema, per_epoch_callback)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms, utils\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "class DDPM(nn.Module):\n",
        "\n",
        "    def __init__(self, network, T=100, beta_1=1e-4, beta_T=2e-2):\n",
        "        \"\"\"\n",
        "        Initialize Denoising Diffusion Probabilistic Model\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        network: nn.Module\n",
        "            The inner neural network used by the diffusion process. Typically a Unet.\n",
        "        beta_1: float\n",
        "            beta_t value at t=1\n",
        "        beta_T: [float]\n",
        "            beta_t value at t=T (last step)\n",
        "        T: int\n",
        "            The number of diffusion steps.\n",
        "        \"\"\"\n",
        "\n",
        "        super(DDPM, self).__init__()\n",
        "\n",
        "        # Normalize time input before evaluating neural network\n",
        "        # Reshape input into image format and normalize time value before sending it to network model\n",
        "        self._network = network\n",
        "        self.network = lambda x, t: (self._network(x.reshape(-1, 1, 28, 28),\n",
        "                                                   (t.squeeze()/T))\n",
        "                                    ).reshape(-1, 28*28)\n",
        "\n",
        "        # Total number of time steps\n",
        "        self.T = T\n",
        "\n",
        "        # Registering as buffers to ensure they get transferred to the GPU automatically\n",
        "        self.register_buffer(\"beta\", torch.linspace(beta_1, beta_T, T+1))\n",
        "        self.register_buffer(\"alpha\", 1-self.beta)\n",
        "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
        "\n",
        "\n",
        "    def forward_diffusion(self, x0, t, epsilon):\n",
        "        '''\n",
        "        q(x_t | x_0)\n",
        "        Forward diffusion from an input datapoint x0 to an xt at timestep t, provided a N(0,1) noise sample epsilon.\n",
        "        Note that we can do this operation in a single step\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x0: torch.tensor\n",
        "            x value at t=0 (an input image)\n",
        "        t: int\n",
        "            step index\n",
        "        epsilon:\n",
        "            noise sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.tensor\n",
        "            image at timestep t\n",
        "        '''\n",
        "\n",
        "        mean = torch.sqrt(self.alpha_bar[t])*x0\n",
        "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
        "\n",
        "        return mean + std*epsilon\n",
        "\n",
        "    def reverse_diffusion(self, xt, t, epsilon):\n",
        "        \"\"\"\n",
        "        p(x_{t-1} | x_t)\n",
        "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xt: torch.tensor\n",
        "            x value at step t\n",
        "        t: int\n",
        "            step index\n",
        "        epsilon:\n",
        "            noise sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.tensor\n",
        "            image at timestep t-1\n",
        "        \"\"\"\n",
        "\n",
        "        mean =  self.network(xt,t)\n",
        "        std = torch.where(t>0, torch.sqrt(((1-self.alpha_bar[t-1]) / (1-self.alpha_bar[t]))*self.beta[t]), 0)\n",
        "\n",
        "        return mean + std*epsilon\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, shape):\n",
        "        \"\"\"\n",
        "        Sample from diffusion model (Algorithm 2 in Ho et al, 2020)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shape: tuple\n",
        "            Specify shape of sampled output. For MNIST: (nsamples, 28*28)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.tensor\n",
        "            sampled image\n",
        "        \"\"\"\n",
        "\n",
        "        # Sample xT: Gaussian noise\n",
        "        xT = torch.randn(shape).to(self.beta.device)\n",
        "\n",
        "        xt = xT\n",
        "        for t in range(self.T, 0, -1):\n",
        "            noise = torch.randn_like(xT) if t > 1 else 0\n",
        "            t = torch.tensor(t).expand(xt.shape[0], 1).to(self.beta.device)\n",
        "            xt = self.reverse_diffusion(xt, t, noise)\n",
        "\n",
        "        return xt\n",
        "\n",
        "\n",
        "    def elbo_simple(self, x0):\n",
        "        \"\"\"\n",
        "        ELBO training objective (Algorithm 1 in Ho et al, 2020)\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x0: torch.tensor\n",
        "            Input image\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            ELBO value\n",
        "        \"\"\"\n",
        "\n",
        "        # Sample time step t\n",
        "        t = torch.randint(1, self.T, (x0.shape[0],1)).to(x0.device)\n",
        "\n",
        "        # Sample noise\n",
        "        epsilon = torch.randn_like(x0)\n",
        "\n",
        "        # TODO: Forward diffusion to produce image at step t\n",
        "        xt = self.forward_diffusion(x0, t, epsilon)\n",
        "\n",
        "        # Sample mean\n",
        "        mean = torch.where(t>0, (torch.sqrt(self.alpha[t])*(1-self.alpha_bar[t-1])/(1-self.alpha_bar[t]))*xt + torch.sqrt(self.alpha_bar[t-1])*self.beta[t]/(1-self.alpha_bar[t])*self.network(xt,t), 0)\n",
        "\n",
        "        return -nn.MSELoss(reduction='mean')(mean, self.network(xt, t))\n",
        "\n",
        "\n",
        "    def loss(self, x0):\n",
        "        \"\"\"\n",
        "        Loss function. Just the negative of the ELBO.\n",
        "        \"\"\"\n",
        "        return -self.elbo_simple(x0).mean()\n",
        "\n",
        "\n",
        "def train(model, optimizer, scheduler, dataloader, epochs, device, ema=True, per_epoch_callback=None):\n",
        "    \"\"\"\n",
        "    Training loop\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: nn.Module\n",
        "        Pytorch model\n",
        "    optimizer: optim.Optimizer\n",
        "        Pytorch optimizer to be used for training\n",
        "    scheduler: optim.LRScheduler\n",
        "        Pytorch learning rate scheduler\n",
        "    dataloader: utils.DataLoader\n",
        "        Pytorch dataloader\n",
        "    epochs: int\n",
        "        Number of epochs to train\n",
        "    device: torch.device\n",
        "        Pytorch device specification\n",
        "    ema: Boolean\n",
        "        Whether to activate Exponential Model Averaging\n",
        "    per_epoch_callback: function\n",
        "        Called at the end of every epoch\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup progress bar\n",
        "    total_steps = len(dataloader)*epochs\n",
        "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
        "\n",
        "    if ema:\n",
        "        ema_global_step_counter = 0\n",
        "        ema_steps = 10\n",
        "        ema_adjust = dataloader.batch_size * ema_steps / epochs\n",
        "        ema_decay = 1.0 - 0.995\n",
        "        ema_alpha = min(1.0, (1.0 - ema_decay) * ema_adjust)\n",
        "        ema_model = ExponentialMovingAverage(model, device=device, decay=1.0 - ema_alpha)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Switch to train mode\n",
        "        model.train()\n",
        "\n",
        "        global_step_counter = 0\n",
        "        for i, (x, _) in enumerate(dataloader):\n",
        "            x = x.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.loss(x)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix(loss=f\"⠀{loss.item():12.4f}\", epoch=f\"{epoch+1}/{epochs}\", lr=f\"{scheduler.get_last_lr()[0]:.2E}\")\n",
        "            progress_bar.update()\n",
        "\n",
        "            if ema:\n",
        "                ema_global_step_counter += 1\n",
        "                if ema_global_step_counter%ema_steps==0:\n",
        "                    ema_model.update_parameters(model)\n",
        "\n",
        "        if per_epoch_callback:\n",
        "            per_epoch_callback(ema_model.module if ema else model)\n",
        "\n",
        "\n",
        "# Parameters\n",
        "T = 1000\n",
        "learning_rate = 1e-3\n",
        "epochs = 5\n",
        "batch_size = 256\n",
        "\n",
        "\n",
        "# Rather than treating MNIST images as discrete objects, as done in Ho et al 2020,\n",
        "# we here treat them as continuous input data, by dequantizing the pixel values (adding noise to the input data)\n",
        "# Also note that we map the 0..255 pixel values to [-1, 1], and that we process the 28x28 pixel values as a flattened 784 tensor.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),    # Dequantize pixel values\n",
        "    transforms.Lambda(lambda x: (x-0.5)*2.0),                    # Map from [0,1] -> [-1, -1]\n",
        "    transforms.Lambda(lambda x: x.flatten())\n",
        "])\n",
        "\n",
        "# Download and transform train dataset\n",
        "dataloader_train = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data', download=True, train=True, transform=transform),\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=True)\n",
        "\n",
        "# Select device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Construct Unet\n",
        "# The original ScoreNet expects a function with std for all the\n",
        "# different noise levels, such that the output can be rescaled.\n",
        "# Since we are predicting the noise (rather than the score), we\n",
        "# ignore this rescaling and just set std=1 for all t.\n",
        "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
        "\n",
        "# Construct model\n",
        "model = DDPM(mnist_unet, T=T).to(device)\n",
        "\n",
        "# Construct optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Setup simple scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
        "\n",
        "\n",
        "def reporter(model):\n",
        "    \"\"\"Callback function used for plotting images during training\"\"\"\n",
        "\n",
        "    # Switch to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        nsamples = 10\n",
        "        samples = model.sample((nsamples,28*28)).cpu()\n",
        "\n",
        "        # Map pixel values back from [-1,1] to [0,1]\n",
        "        samples = (samples+1)/2\n",
        "        samples = samples.clamp(0.0, 1.0)\n",
        "\n",
        "        # Plot in grid\n",
        "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=nsamples)\n",
        "        plt.gca().set_axis_off()\n",
        "        plt.imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
        "        plt.show()\n",
        "\n",
        "# Call training loop\n",
        "train(model, optimizer, scheduler, dataloader_train,\n",
        "      epochs=epochs, device=device, ema=True, per_epoch_callback=reporter)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29502e060dbb4b0db2feda3b2b9f0600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a92fdede28114771868307c2695f1014",
              "IPY_MODEL_1d8c515548034a26acb475a21e49ad99",
              "IPY_MODEL_3510307ac24b449ab9853ecf2661d119"
            ],
            "layout": "IPY_MODEL_3631c39d8fc04c0e8f1830c150be168b"
          }
        },
        "a92fdede28114771868307c2695f1014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_098ad33be85c4027b6bae3cb2dc33b67",
            "placeholder": "​",
            "style": "IPY_MODEL_a2510616091d44779a4072e101b4a4c2",
            "value": "Training:   2%"
          }
        },
        "1d8c515548034a26acb475a21e49ad99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06fe2a00be72439cad2f0a6586966fde",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdd86dff33a44d7e8ca4134463933af1",
            "value": 24
          }
        },
        "3510307ac24b449ab9853ecf2661d119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ddedb88f44a452dafbea68fac60d427",
            "placeholder": "​",
            "style": "IPY_MODEL_5b80b05a8733492f9bf3fc8a350bfa05",
            "value": " 24/1175 [01:10&lt;53:25,  2.79s/it, epoch=1/5, loss=⠀      0.4847, lr=9.98E-04]"
          }
        },
        "3631c39d8fc04c0e8f1830c150be168b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "098ad33be85c4027b6bae3cb2dc33b67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2510616091d44779a4072e101b4a4c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06fe2a00be72439cad2f0a6586966fde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdd86dff33a44d7e8ca4134463933af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ddedb88f44a452dafbea68fac60d427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b80b05a8733492f9bf3fc8a350bfa05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}